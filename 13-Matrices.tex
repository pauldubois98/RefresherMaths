\chapter{Matrices}
\textbf{Representation of a finite-dimensional linear map by a matrix}\\
$f:V_1 \to V_2$, $B_1=\{x_1,\dots,x_n\},B_2=\{y_1,\dots,y_m\}$ basis for $V_1,V_2$ respectively.
As $f$ is linear, there are coefficients $\{a_{ij} \mid 1 \leq i \leq m, 1 \leq j \leq n\}$ such that: $f(x_j)=\sum_{i=1}^{m}a_{ij}y_i$
Then, we put 
$$
M_{B_1,B_2}(f)=
\begin{pmatrix}
	a_{11} & a_{12} & \dots  & a_{1n} \\
	a_{21} & a_{22} & \dots  & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \dots  & a_{mn}
\end{pmatrix}
$$
A matrix can always be interpreted as the representation of a linear map. This interpretation is used extensively in the proofs of linear algebra. But this interpretation may be useless in some other contexts, e.g. when the matrix is just used to store data.

\textbf{The spaces of matrices and linear maps are isomorphic}\\
There is a one-to-one correspondence between $n \times m$ matrices and linear maps from a vector space of dimension $m$ to a vector space of dimension $n$.

\textit{Multiplication of a column vector by a matrix}
[Draw it \& Explain]

\textit{Multiplication of two matrices}
[Draw it \& Explain]\\
Note: multiplication of matrices is \textbf{not} commutative, and is defined only for matching dimensions.

Example of non-commutativity:
$$
\begin{pmatrix}
	0&1\\
	1&0
\end{pmatrix}
\begin{pmatrix}
	1&0\\
	0&2
\end{pmatrix}
=
\begin{pmatrix}
	0&1\\
	2&0
\end{pmatrix}
$$
$$
\begin{pmatrix}
	0&1\\
	1&0
\end{pmatrix}
\begin{pmatrix}
	2&0\\
	0&1
\end{pmatrix}
=
\begin{pmatrix}
	0&2\\
	1&0
\end{pmatrix}
$$

\textit{Associativity of the matrix product}
\begin{itemize}
	\item $(\lambda A)B = \lambda(AB) = A(\lambda B)$
	\item $(AB)C = A(BC)$
\end{itemize}

\textbf{Relation between the composition of linear maps and the multiplication of matrices}\\
$f:V_1 \to V_2, g:V_2 \to V_3$, $B_1,B_2,B_3$ basis for $V_1,V_2,V_3$ respectively.
$$
M_{B_1,B_3}(f \circ g)=M_{B_1,B_2}(f)M_{B_2,B_3}(g)
$$

\textit{The identity matrix}
\begin{definition}[Identity Matrix]
	$I_n$ is the $n \times n$ matrix with all entries being $0$ except on the diagonal, where entries are $1$.\\
	e.g.:
	$$
	I_2 = 
	\begin{pmatrix}
		1&0\\
		0&1
	\end{pmatrix}
	$$
\end{definition}
\begin{property}[Multiplication by the Identity Matrix]
	$A$ a $n \times n$ matrix:\\
	$A I_n = I_n A = A$
\end{property}

\begin{exercise}
	Represent in the canonical basis of $\R^2$ the linear map $r_\alpha: \R^2 \to \R^2$ that rotates any vector by an angle of $\alpha$ anticlockwise.
\end{exercise}

\textit{Inverse of a square matrix}
\begin{definition}
	$A$ a $n \times n$ matrix:\\
	$B$ is the inverse of $A$ if $AB = I_n = BA$; it is then denoted $A^{-1}$
\end{definition}
\begin{question}
	What is the inverse of:
	\begin{itemize}
		\item $I_n$
		\item $R_\alpha$
		\item $\begin{pmatrix} 0&1\\ 1&0 \end{pmatrix}$
		\item $\begin{pmatrix} 1&0\\ 0&0 \end{pmatrix}$
	\end{itemize}
\end{question}

\textit{Why searching the inverse of a matrix ?}
Common problem: find $x$ such that $Ax=b$. This is not easy by itself, but can be solved easily if we know $A^{-1}$: $x=A^{-1}b$.

\textit{Relation between inverse function and inverse matrix:}
\begin{property}
	$f:V_1 \to V_2$ with inverse $f^{-1}:V_2 \to V_1$:
	$M_{B_2,B_1}(f^{-1}) = M_{B_1,B_2}(f)^{-1}$
\end{property}

\textit{How to compute the inverse of a matrix in practice?}
\begin{itemize}
	\item $2 \times 2$ matrices: explicit formula
	\item Gauss-Jordan elimination
	\item LU decomposition
\end{itemize}

In general, inverting a matrix takes $\mathcal{O}(n^3)$ operations for an $n \times n$ matrix.
\section{Determinant}
Want to know when a matrix is invertible... Check if determinant is zero!
\begin{itemize}
	\item $2 \times 2$ matrices: explicit formula
	\item $3 \times 3$ matrices: explicit formula
	%\item Formal definition?
	\item Expanding through row/columns
\end{itemize}
\begin{definition}[Characteristic polynomial]
	The charactristic polynomial of $M$ is $P(x)=\det{xI-M}$.
\end{definition}


\section{Rank of a Matrix}
This is the rank of the associated linear map.
In practice, this is just the number of linearly independent rows.

\begin{definition}[Linearly Independent Rows]
	A set of rows is linearly independent is none of the rows can be expressed linearly in terms of the others.
\end{definition}
\begin{definition}[Full Rank]
	A matrix has full rank if its rank equals its number of rows.\\
	In other terms, a matrix has full rank if all its rows are linearly independent.
\end{definition}
If a matrix has full rank, then the matrix is invertible (if it is square).

\section{Eigenvalues \& Eigenvectors}
\begin{definition}[Eigenvalues \& Eigenvectors]
	With $M$ an $n \times n$ matrix:
	$u$ is an eigenvector of $M$ with associated eigen-value $\lambda$ if $Mu=\lambda u$.
\end{definition}

Eigenvalues are roots of characteristic polynomial.
Then solve $(\lambda I-M)u=0$ to find the eigenvector $u$ associated with $\lambda$.

\begin{example}
	Compute eigenvalues and eigenvectors of $\begin{pmatrix} 4&1\\ 6&3 \end{pmatrix}$
\end{example}

\section{Diagonalization}
\begin{definition}[Diagonal Matrix]
	$M$ is diagonal if all non-zero entries are on its diagonal.
\end{definition}

Method: Solve $(I-M)v=0$
Let $P=(v_1 v_2 \dots v_n)$.
Then $P^{-1}AP = D$ where $D$ is diagonal (and in fact composed of the eigenvalues).

\begin{example}
	Compute diagonalization of $\begin{pmatrix} 4&1\\ 6&3 \end{pmatrix}$ \& raise it to the power 10.
\end{example}

