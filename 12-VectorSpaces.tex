\chapter{Vector Spaces}
\section{Axioms}
\begin{definition}[Vector Space]
	$\F$ a field (usually $\R$ or $\C$).
	$V$ is a vector field over $\F$ if:
	\begin{itemize}
		\item $\forall v,w \in V: v+w \in V$ "$V$ has addition"
		\item $\forall v \in V, k \in \F: k.v \in V$ "$V$ has multiplication by a scalar"
	\end{itemize}
	Such that:
	\begin{itemize}
		\item $\forall v \in V, k,l \in \F: (kl).v = k.(l.v)$
		\item $\forall v \in V, k,l \in \F: (k+l).v = k.v + l.v$
		\item $\forall v,w \in V, k \in \F: k.(v+w) = k.v + k.w$
		\item $\forall v \in V: 1.v=v$
		\item $\forall v \in V: 0.v=\textbf{0}$
		\item $\forall k \in \F: k.\textbf{0}=\textbf{0}$
	\end{itemize}
\end{definition}
\begin{example}
	\begin{itemize}
		\item $0$ over $\R$
		\item $\R$ over $\R$
		\item $\C$ over $\R$
		\item $\C$ over $\C$
		\item $\R^n$ over $\R$
		\item $\R\left[x\right]$ over $\R$ (real polynomials)
		\item $\R^{\N}$ over $\R$ (real sequences)
		\item $\R^{\R}$ over $\R$ (real functions)
		\item $V_1 \times V_2$ if $V_1$ and $V_2$ are vector spaces over the same field $\F$
	\end{itemize}
\end{example}

\begin{definition}[Vector Sub-space]
	$W \subseteq V$ is a vector subspace of $V$ if it is a vector space on its own.
	Need to check:
	\begin{itemize}
		\item Closed under addition: $w,w' \in W \implies w+w' \in W$
		\item Multiplication by a scalar: $w \in W, k \in \F \implies k.w \in W$
		\item Contains the null vector: $\textbf{0} \in W$
	\end{itemize}
\end{definition}
\begin{example}
	\begin{itemize}
		\item $0$ is a vector sub-space of $\R$
		\item $\R$ is a vector sub-space of $\C$ over $\R$
		\item $\R_d\left[x\right]$ is a vector sub-space of $\R\left[x\right]$ over $\R$ (real polynomials of degree $d$)
		\item $C\left[ \R\right]$ is a vector sub-space of $\R^{\R}$ over $\R$ (real functions)
	\end{itemize}
\end{example}
\begin{proposition}
	$W_1, W_2 \text{ subspaces of } V \implies W_1 \cap W_2 \text{ subspace of } V$
\end{proposition}
\begin{proof}
	in problem set
\end{proof}

\begin{property}[Direct product of vector spaces are vector spaces]
	$V_1, V_2 \text{ vector spaces } \implies V_1 \times V_2 \text{ is a vector space}$\\
	$(v_1,v_2),(v_1',v_2') \in V_1 \times V_2, k \in \F$:
	\begin{itemize}
		\item $(v_1,v_2)+(v_1',v_2')=(v_1+v_1',v_2+v_2')$
		\item $k.(v_1,v_2)=(k.v_1,k.v_2)$
	\end{itemize}
\end{property}

\begin{definition}[Linear combination]
	$A \subset V \text{ vector space}$: $x$ is a linear combination of $A$ iff $\exists k \in \N \text{ s.t. } \exists k_1,\dots,k_n \in \F, x_1,\dots,x_n \in A \text{ s.t. } x=\sum_{i=1}^{n}k_i.x_i$
\end{definition}
\begin{definition}[Span]
	$A \subset V \text{ vector space}$: $\Span{A}$ is the set of all vector that can be expressed as a linear combination of $A$, i.e. $\Span{A}=\{ \sum_{i=1}^{n}k_i.x_i \mid n\in \N, k_i \in \F, x_i\in A \}$
\end{definition}
\begin{example}
	\begin{itemize}
		\item $\Span{\{\textbf{1}\}}=\R$
		\item $\Span{\{\textbf{0}\}}=\{\textbf{0}\}$
		\item $\Span{\{\textbf{1},\textbf{i}\}}=\C$
	\end{itemize}
\end{example}
\begin{property}
	$\Span{A}$ is the smallest vector space containing $A$
\end{property}
\begin{proof}
	any smaller set containing $A$ is not closed under addition/multiplication by scalar
\end{proof}
\begin{exercise}
	Which of these can be seen as vector spaces?
	\begin{itemize}
		\item $\R^{\N}$
		\item $\{ (x,y) \mid x^2+y^2 \leq 1 \}$ "unit disk"
		\item $\{ (x,y) \mid x+y=0 \}$
		\item $\{ (x,y) \mid x+y=1 \}$
		\item $\{ f \in C\left[ a,b \right] \mid f(a)=f(b) \}$
	\end{itemize}
\end{exercise}



\section{Dimension \& Basis}
\begin{definition}[Linear Independence]
	$x_1,\dots,x_n$ are linearly independent (LI) if $\forall k_1,\dots,k_n \in \F, \sum_{i=1}^{n}k_i.x_i = 0 \implies \forall 1 \leq i \leq n, k_i=0$
\end{definition}
\begin{example}
	\begin{itemize}
		\item $(1,0,0)$ \& $(0,1,0)$ in $\R^3$
		\item $1$ \& $i$ in $\C$
		\item $(1,0,1)$, $(5,0,1)$ \& $(1,3,0)$ in $\R^3$
	\end{itemize}
\end{example}
\begin{definition}[Basis of a Vector Space]
	$V$ has basis $B \subset V$ if $\Span{B}=V$ and $B$ is LI.
\end{definition}
\begin{example}
	\begin{itemize}
		\item $\{(1,0,0), (0,1,0), (0,0,1)\}$ in $\R^3$
		\item $\{1,i\}$ in $\C$
	\end{itemize}
\end{example}
The LI property tends to make the basis "small", while the spanning property tends to make it "large".\\
A basis is a largest LI set, or a smaller spanning set.
\begin{property}[Basis always exist]
	Any vector space $V$ has a basis
\end{property}
\begin{proof}
	Either remove from a spanning set (e.g. $V$ spans itself), or (in finite dimensions), add elements to LI set until the set spans all of $V$.
\end{proof}
\begin{property}[All Basis have the Same Number of Elements]
	If $B$ and $B'$ are both basis of $V$ (i.e. span $V$ and are LI), then $\card{B}=\card{B'}$
\end{property}
\begin{proof}
	Technical, omitted
\end{proof}

\begin{definition}[Dimension]
	The dimension of a vector space is the cardinal of a basis (note it ay be finite of infinite)
\end{definition}
\begin{example}
	\begin{itemize}
		\item $\R^3$ has dimension 3 (finite)
		\item $\R\left[ x \right]$ has infinite dimension
		\item $\dim_\R(\C)=2$
		\item $\dim_\C(\C)=1$
		\item $\dim(\R^n)=n$
		\item $\dim(\{\textbf{0}\})=n$
		
	\end{itemize}
\end{example}
\begin{property}[Algebra of Dimensions]
	\begin{itemize}
		\item $\dim(V \times V')=\dim(V)+\dim(V')$
		\item $\dim(V + V')=\dim(V)+\dim(V')-\dim(V \cap V')$
	\end{itemize}
\end{property}

\section{Linear Maps}
Here, $V$ and $W$ are vector spaces over $\F$.
\begin{definition}[Linear Map]
	$f: V \to W$ is linear if:
	\begin{itemize}
		\item $f(v+v')=f(v)+f(v') \forall v,v' \in V$
		\item $f(k.v)=k.f(v) \forall v \in V, k \in \F$
	\end{itemize}
\end{definition}
\begin{definition}
	With $f: V \to W$:
	\begin{itemize}
		\item $f$ is an endomorphism if $V=W$
		\item $f$ is an isomorphism if $f$ is bijective
		\item $f$ is an automorphism if $V=W$ and $f$ is bijective
		\item $f$ is a linear form if $W=\F$
	\end{itemize}
\end{definition}
\begin{example}
	\begin{itemize}
		\item $\R \ni x \mapsto ax \in \R$ "classic linear map"
		\item $\R\left[ x \right] \ni P \mapsto P' \in \R\left[ x \right]$ "derivative"
		\item $\R^\R \ni f \mapsto f(0) \in \R$ "evaluation at 0"
	\end{itemize}
\end{example}
\begin{property}
	$f \text{linear} \implies f(0)=0$
\end{property}
\begin{proof}
	exercise
\end{proof}
\begin{property}
	$f: V \to W$ linear: $\Image{f}$ is a subspace of $W$, and $f^{-1}(W')$ is a subspace of $V$ if $W'$ is a subspace of $W$
\end{property}
\begin{proof}
	exercise
\end{proof}

\begin{definition}[Kernel]
	$\Ker{f}=f^{-1}(\{0\})$
\end{definition}
\begin{property}[Injectivity for Linear Functions]
	$f$ linear: $f$ is injective iff $\Ker{f}=\{0\}$
\end{property}
\begin{proof}
	prove both directions, easy from def
\end{proof}

\begin{property}[Composition of Linear Maps]
	$f:V_1 \to V_2, g:V_2 \to V_3 \text{ linear maps } \implies f \circ g:V_1 \to V_3 \text{ is a linear map}$
\end{property}
\begin{proof}
	easy from definition
\end{proof}
\begin{property}[Inverse of a Linear Map]
	$f:V_1 \to V_2 \text{ linear map with inverse} \implies f^{-1}:V_2 \to V_1 \text{ is a linear map}$
\end{property}
\begin{proof}
	easy from definition
\end{proof}

\begin{property}
	$f:V_1 \to V_2 \text{ isomorphism}, B \text{ basis for } V_1 \implies f(B) \text{ is a basis for } V_2$ 
\end{property}
\begin{proof}
	Use injectivity of $f$ to show $f(B)$ is LI, and surjectivity of $f$ to show $f(B)$ spans $V_2$.
\end{proof}
\begin{notation}[Isomorphic Spaces]
	$V_1 \equiv V_2 \iff \exists f:V_1 \to V_2 \text{ isomorphic}$
\end{notation}
\begin{property}
	A linear map is fully determined by its action on a basis.
\end{property}
\begin{proof}
	using linearity
\end{proof}
\begin{property}[Dimension of Isomorphic Spaces]
	$V_1 \equiv V_2 \iff \dim(V_1)=\dim(V_2)$ (in finite dimensions)
\end{property}
\begin{proof}
	$\implies$ use isomorphism to transform basis; $\impliedby$ basis are of the same size, create a linear isomorphism from one basis to the other
\end{proof}

\begin{definition}[Rank of a Linear Map]
	$\Rank{f}=\dim(\Image{f})$
\end{definition}
\begin{definition}[Nullity of a Linear Map]
	$\Nullity{f}=\dim(\Ker{f})$
\end{definition}
\begin{theorem}[Kernel-Rank/Nullity-Rank]
	$f:V \to W$ linear: $\Rank{f}+\Nullity{f}=\dim(V)$
\end{theorem}
\begin{proof}
	Let $V_0$ a subspace such that $V_0 \oplus \Ker{f} = V$.
	Then $g:x \in V_0 \to f(x) \in \Image{f}$ is an isomorphism.
	$g$ is clearly injective, since $\Ker{g}=\Ker{f} \cap V_0 = \{0\}$
	And $g$ is surjective since for any $y \in \Image{f}$, there exists $x \in V$ such that $f(x)=y$ and $x$ is uniquely decomposed into $x_1 + x_2$ with $x_1 \in V_0$ and $x_2 \in \Ker{f}$, so $f(x_1+x_2)=f(x_1)=g(x_1)=y$.

\end{proof}

\section{Link with matrices}
\textbf{Representation of a finite-dimensional linear map by a matrix}\\
$f:V_1 \to V_2$, $B_1=\{x_1,\dots,x_n\},B_2=\{y_1,\dots,y_m\}$ basis for $V_1,V_2$ respectively.
As $f$ is linear, there are coefficients $\{a_{ij} \mid 1 \leq i \leq m, 1 \leq j \leq n\}$ such that: $f(x_j)=\sum_{i=1}^{m}a_{ij}y_i$
Then, we put 
$$
M_{B_1,B_2}(f)=
\begin{pmatrix}
	a_{11} & a_{12} & \dots  & a_{1n} \\
	a_{21} & a_{22} & \dots  & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \dots  & a_{mn}
\end{pmatrix}
$$
A matrix can always be interpreted as the representation of a linear map. This interpretation is used extensively in the proofs of linear algebra. But this interpretation may be useless in some other contexts, e.g. when the matrix is just used to store data.
\begin{exercise}
	Represent in the canonical basis of $\R^2$ the linear map $r_\alpha: \R^2 \to \R^2$ that rotates any vector by an angle of $\alpha$ anticlockwise.
\end{exercise}

\textbf{The spaces of matrices and linear maps are isomorphic}\\
There is a one-to-one correspondence between $n \times m$ matrices and linear maps from a vector space of dimension $m$ to a vector space of dimension $n$.

\textit{Multiplication of a column vector by a matrix}
[Draw it \& Explain]

\textit{Multiplication of two matrices}
[Draw it \& Explain]\\
Note: multiplication of matrices is \textbf{not} commutative, and is defined only for matching dimensions.

Example of non-commutativity:
$$
\begin{pmatrix}
	0&1\\
	1&0
\end{pmatrix}
\begin{pmatrix}
	1&0\\
	0&2
\end{pmatrix}
=
\begin{pmatrix}
	0&1\\
	2&0
\end{pmatrix}
$$
$$
\begin{pmatrix}
	0&1\\
	1&0
\end{pmatrix}
\begin{pmatrix}
	2&0\\
	0&1
\end{pmatrix}
=
\begin{pmatrix}
	0&2\\
	1&0
\end{pmatrix}
$$

\textit{Associativity of the matrix product}
\begin{itemize}
	\item $(\lambda A)B = \lambda(AB) = A(\lambda B)$
	\item $(AB)C = A(BC)$
\end{itemize}

\textbf{Relation between the composition of linear maps and the multiplication of matrices}\\
$f:V_1 \to V_2, g:V_2 \to V_3$, $B_1,B_2,B_3$ basis for $V_1,V_2,V_3$ respectively.
$$
M_{B_1,B_3}(f \circ g)=M_{B_1,B_2}(f)M_{B_2,B_3}(g)
$$

\textit{The identity matrix}
\begin{definition}[Identity Matrix]
	$I_n$ is the $n \times n$ matrix with all entries being $0$ except on the diagonal, where entries are $1$.\\
	e.g.:
	$$
	I_2 = 
	\begin{pmatrix}
		1&0\\
		0&1
	\end{pmatrix}
	$$
\end{definition}
\begin{property}[Multiplication by the Identity Matrix]
	$A$ a $n \times n$ matrix:\\
	$A I_n = I_n A = A$
\end{property}

\textit{Inverse of a square matrix}
\begin{definition}
	$A$ a $n \times n$ matrix:\\
	$B$ is the inverse of $A$ if $AB = I_n = BA$; it is then denoted $A^{-1}$
\end{definition}
\begin{question}
	What is the inverse of:
	\begin{itemize}
		\item $I_n$
		\item $R_\alpha$
		\item $\begin{pmatrix} 0&1\\ 1&0 \end{pmatrix}$
		\item $\begin{pmatrix} 1&0\\ 0&0 \end{pmatrix}$
	\end{itemize}
\end{question}

\textit{Why searching the inverse of a matrix ?}
Common problem: find $x$ such that $Ax=b$. This is not easy by itself, but can be solved easily if we know $A^{-1}$: $x=A^{-1}b$.

\textit{Relation between inverse function and inverse matrix:}
\begin{property}
	$f:V_1 \to V_2$ with inverse $f^{-1}:V_2 \to V_1$:
	$M_{B_2,B_1}(f^{-1}) = M_{B_1,B_2}(f)^{-1}$
\end{property}

\textit{How to compute the inverse of a matrix in practice?}
\begin{itemize}
	\item $2 \times 2$ matrices: explicit formula
	\item Gauss-Jordan elimination
	\item LU decomposition
\end{itemize}

In general, inverting a matrix takes $\mathcal{O}(n^3)$ operations for an $n \times n$ matrix.


